Oded Herzberg

# Insurance Claim Information Retrieval System

**Midterm Coding Assignment - GenAI + Multi-Agent Orchestration**

A multi-agent RAG system for answering questions about insurance claims using specialized retrieval strategies.

---

## ğŸ“‹ Table of Contents

1. [Project Overview](#project-overview)
2. [System Architecture](#system-architecture)
3. [Indexing Strategy](#indexing-strategy)
4. [Agent System](#agent-system)
5. [MCP Discussion](#mcp-discussion)
6. [Evaluation](#evaluation)
7. [QA Testing Suite](#qa-testing-suite)
8. [Quick Start](#quick-start)

---

## ğŸ“– Project Overview

This system answers questions about insurance claims using three specialized agents:
- **Router Agent**:  Classifies questions as summary or needle queries
- **Needle Agent**:  Finds specific facts using small chunk retrieval
- **Summary Agent**: Provides overviews using page-level summaries

### Key Features

- 10-page insurance claim document with chronological information
- Needle index with auto-merge for adaptive context
- Summary index with always-include overview pages
- Multi-agent orchestration with GPT-4o-mini
- RAGAS evaluation with Gemini as judge

### Technology Stack

- **LLM**: OpenAI GPT-4o-mini
- **Embeddings**: OpenAI text-embedding-3-small (1536 dims)
- **Database**: Supabase (PostgreSQL + pgvector)
- **Evaluation**: RAGAS + Google Gemini
- **Framework**: LlamaIndex

---

## ğŸ—ï¸ System Architecture

### Flow Diagram

```
User Query
    â†“
Routing Agent (Classify: SUMMARY or NEEDLE)
    â†“
    â”œâ”€â†’ [NEEDLE] â†’ Needle Agent â†’ Vector Search (Supabase)
    â”‚                                â†“
    â”‚                    Retrieve top-6 similar chunks
    â”‚                                â†“
    â”‚                    Auto-Merge (if â‰¥3 chunks from same parent)
    â”‚                                â†“
    â”‚                    Add full parent page from docstore to context
    â”‚
    â””â”€â†’ [SUMMARY] â†’ Summary Agent â†’ Vector Search (Supabase)
                                â†“
                    Retrieve page summaries: Overview (1, 10) + top-4 of detailed pages
    â†“
Final Answer (GPT-4o-mini)
```

### System Components

```
Project/
â”œâ”€â”€ Data/                      # Document generation (10-page PDF)
â”œâ”€â”€ Indexing/                  # Needle & summary indexes
â”œâ”€â”€ Agents/                    # Router, needle, summary agents
â”œâ”€â”€ Evaluation/                # RAGAS framework
â”œâ”€â”€ Config/                    # System parameters
â””â”€â”€ main.py                    # Interactive menu
```

---

## ğŸ“Š Indexing Strategy

### Document Structure

**File**: `Data/insurance_claim.pdf` (10 pages)

- **Timeline**:     January 15 â€“ February 5, 2024
- **Page Types**:   Overview (Pages 1, 10) + Details (Pages 2-9)
- **Metadata**:     Header, date, involved parties, page type
- **Precision**:    Second-level timestamps (e.g., 09:23:45 AM)

### 1. Needle Index (For Specific Facts)

**Purpose**: Retrieve precise information using small chunks

**Strategy**:

| Parameter     | Value              | Rationale 
|---------------|--------------------|-----------
| Chunk Size    | 400 chars          | Balances precision with context 
| Overlap       | 50 chars           | Preserves sentence continuity across chunks 
| Hierarchy     | 2 levels           | Chunks â†’ Parent pages 
| Auto-Merge    | â‰¥3 chunks          | Adds parent if multiple chunks from same page
| Metadata      | Date, header, type | Provides temporal/structural context 
| Min Chunk     | 200 chars          | Merges tail chunks if too small


**How It Works**:
1. Split each page into 400-character chunks with 50-char overlap (sentence-aware)
2. Merge tail chunks smaller than 200 chars into previous chunk
3. Store chunks with embeddings in Supabase
4. On query: Retrieve top-6 similar chunks
5. If â‰¥3 chunks from same parent page â†’ Add full parent for context
6. Pass chunks (+ parent if merged) to LLM


**Why 400-Character Chunks?**
- âœ… High precision for specific facts
- âœ… Less noise in retrieval
- âœ… Better semantic matching
- âœ… Adequate context for most queries
- âœ… Sentence-aware overlap maintains coherence
- âš ï¸ May lack broader context (solved by auto-merge)

**Example**: Query "What was Sarah's blood pressure?" retrieves the exact medical assessment chunk.

### 2. Summary Index (For Overviews)

**Purpose**: Answer high-level questions using page summaries

**Strategy**:
- **Map**:               Summarize each page independently (GPT-4o-mini)
- **Reduce**:            Store page-level summaries with embeddings
- **Always Include**:    Overview pages (1, 10) regardless of similarity
- **Retrieval**:         Overview pages + top-4 similar detail pages

**Why Always Include Overview?**
- Page 1: Claim basics (parties, dates, amount)
- Page 10: Final resolution
- Vector similarity alone might miss these critical pages

**Example**: Query "What was the total claim value?" includes Pages 1, 10 + top financial detail pages.

### 3. Vector Similarity

- **Embedding Model**: `text-embedding-3-small`
- **Similarity**:       Cosine similarity
- **Top-K**:            Configurable (Needle: 6, Summary: 6)
- **Storage**:          Supabase with pgvector extension

---

## ğŸ¤– Agent System

### 1. Routing Agent

**Purpose**: Classify queries as SUMMARY or NEEDLE

**LLM**: GPT-4o-mini

**Logic**:
- **SUMMARY**: High-level, overview, timeline questions
- **NEEDLE**:  Specific facts, precise details

**Method**: Example-based prompt with keywords

### 2. Needle Agent

**Purpose**: Answer precise factual questions

**Process**:
1. Embed user query
2. Vector search in chunks (cosine similarity)
3. Retrieve top-6 chunks
4. Check auto-merge: â‰¥3 chunks from same parent?
5. Build context (chunks + parent if merged)
6. Generate answer with GPT-4o-mini

**Output Style**: Concise, factual, natural language (no "chunk" references)

**Example**:
```
Q: "How many feet were the skid marks?"
A: "The skid marks at the collision scene measured exactly 47 feet 
    from Chen's vehicle trajectory on the wet pavement."
```

### 3. Summary Agent

**Purpose**: Answer high-level overview questions

**Process**:
1. Embed user query
2. Retrieve ALL Overview pages (always)
3. Vector search in Detail pages
4. Combine: Overview + top-4 Detail pages
5. Generate answer with GPT-4o-mini (3-5 sentences)

**Output Style**: Concise but complete, includes key supporting facts

**Example**:
```
Q: "What was the final resolution?"
A: "The claim was resolved on February 5, 2024, with a total payout 
    of $47,850. Sarah Mitchell received $43,510 for vehicle damage 
    and $3,840 for medical expenses. The vehicle was repaired and 
    passed quality inspection."
```

---

## ğŸ”Œ MCP Discussion

The assignment required MCP (Model Context Protocol) integration, but OpenAI's Python SDK doesn't natively support MCP protocol (which requires specific client implementations like Claude Desktop). We implemented direct Supabase SDK integration instead, which provides the same functionality (vector search, metadata retrieval) through a more stable and maintainable architecture for OpenAI-based agents.

---

## ğŸ“ˆ Evaluation

### Framework: RAGAS

Industry-standard RAG evaluation with Gemini as LLM judge (independent from system's OpenAI models).

### Metrics (6 Total)

1. **Context Precision**:   Are retrieved chunks relevant?
2. **Context Recall**:      Were correct chunks retrieved?
3. **Faithfulness**:        Is answer grounded in context?
4. **Answer Relevancy**:    Does answer address the question?
5. **Answer Similarity**:   Semantic match to ground truth?
6. **Answer Correctness**:  Factually accurate?

### Test Suite

- **10 test cases**:        5 needle + 5 summary questions
- **Ground truth**:         Manually created from source document
- **Expected chunks**:      Pre-defined for recall measurement
- **Two phases**:           Query phase â†’ Evaluation phase (efficient, no re-runs)

### Results

**Output Files**:
- `query_results.json`: Agent responses and contexts
- `evaluation_results.json`: RAGAS scores
- `evaluation_report.pdf`: Visual performance report

**Example Scores** (from actual evaluation):
```
Overall System: 0.747
â”œâ”€ Needle Agent: 0.788 (strong precision)
â””â”€ Summary Agent: 0.705 (good synthesis)

Key Strengths:
âœ“ High faithfulness (low hallucination)
âœ“ High similarity (semantically correct)
âœ“ Good answer relevancy
```

---

## âš ï¸ Limitations & Trade-offs

### Current Limitations

1. **Single document** (extensible to multi-document with doc_id metadata)
2. **Fixed chunk size** (could implement adaptive chunking)
3. **No caching** (could add Redis for frequent queries)
4. **Synchronous processing** (could implement async for scale)

### Design Trade-offs

**Chunk Size (400 chars)**:
- Smaller (<300) = Higher precision âœ“, Missing context âš ï¸
- Current (400) = Good balance âœ“, Sentence-aware âœ“
- Larger (>600) = More context âœ“, Lower precision âš ï¸
- **Choice**: Optimal for insurance document granularity

**Auto-Merge Threshold (3)**:
- Low = More context, Higher cost
- High = Less merging, Possible info loss
- **Choice**: Requires strong signal before adding parent

**Summary Style (3-5 sentences)**:
- Brief = Fast, May miss details
- Verbose = Complete, Poor UX
- **Choice**: Concise but complete

---

## ğŸš€ Quick Start

### Prerequisites

- Python 3.9+
- OpenAI API key
- Supabase account (with pgvector)
- Google AI API key (for evaluation)

### Installation

```bash
# Install all dependencies
pip install -r requirements.txt

# Setup environment (.env file)
OPENAI_API_KEY       =your-key
SUPABASE_URL         =your-url
SUPABASE_KEY         =your-key
SUPABASE_DB_PASSWORD =your-password
GOOGLE_AI_API_KEY    =your-key  # Optional (evaluation only)
```

### Setup Database

The system **automatically creates required tables** when you run the indexing process for the first time. No manual SQL setup needed!

The system automatically creates tables with this schema:

```sql
-- claim_chunks: stores small text chunks for needle queries
CREATE TABLE claim_chunks (
  chunk_id TEXT PRIMARY KEY,
  content TEXT NOT NULL,
  embedding VECTOR(1536),
  metadata JSONB,
  page_number INTEGER,
  chunk_index INTEGER,
  parent_id TEXT
);

-- claim_summaries: stores page summaries for overview queries
CREATE TABLE claim_summaries (
  summary_id TEXT PRIMARY KEY,
  content TEXT NOT NULL,
  embedding VECTOR(1536),
  metadata JSONB,
  page_number INTEGER,
  summary_type TEXT
);
```

### Run System

```bash
python main.py
```

**First Time Setup**: Use menu Option 2 to create indexes before asking questions.

**Menu Options**:
1. Ask a Question
2. Create/Recreate Indexing
3. RAGAS Evaluation
4. Exit

### Example Queries

**Needle**:
- "What is the claim number?"
- "What was Sarah's blood pressure?"
- "How many feet were the skid marks?"

**Summary**:
- "What was the total claim value?"
- "Who was at fault and why?"
- "What was the final resolution?"

---

## ğŸ“ Project Structure

```
insurance-claim-rag-system/
â”‚
â”œâ”€â”€ Data/
â”‚   â”œâ”€â”€ generate_claim_pdf.py     # Creates 10-page claim
â”‚   â”œâ”€â”€ insurance_claim.pdf       # Generated document
â”‚   â””â”€â”€ claim_metadata.json       # Page metadata
â”‚
â”œâ”€â”€ Indexing/
â”‚   â”œâ”€â”€ needle_indexing.py        # Needle index
â”‚   â”œâ”€â”€ summary_indexing.py       # Summary index
â”‚   â””â”€â”€ [index files]             # Vector storage
â”‚
â”œâ”€â”€ Agents/
â”‚   â”œâ”€â”€ routing_agent.py          # Query classifier
â”‚   â”œâ”€â”€ needle_agent.py           # Precise retrieval
â”‚   â””â”€â”€ summary_agent.py          # Overview generation
â”‚
â”œâ”€â”€ Evaluation/
â”‚   â”œâ”€â”€ evaluate.py               # RAGAS runner
â”‚   â”œâ”€â”€ test_dataset.json         # 10 test cases
â”‚   â”œâ”€â”€ evaluation_report.pdf     # Performance report
â”‚   â””â”€â”€ [results files]           # JSON outputs
â”‚
â”œâ”€â”€ QA/                            # QA Testing Suite
â”‚   â”œâ”€â”€ test_data/                # Test datasets (60 tests)
â”‚   â”œâ”€â”€ graders/                  # Code/model/HITL graders
â”‚   â”œâ”€â”€ collectors/               # Answer collection
â”‚   â”œâ”€â”€ reporters/                # JSON/PDF reporters
â”‚   â”œâ”€â”€ results/                  # Test outputs
â”‚   â”œâ”€â”€ run_qa_tests.py           # Main test runner
â”‚   â”œâ”€â”€ run_hitl_tests.py         # HITL runner
â”‚   â””â”€â”€ README.md                 # QA documentation
â”‚
â”œâ”€â”€ Config/
â”‚   â””â”€â”€ config.py                 # System parameters
â”‚
â”œâ”€â”€ main.py                        # Main orchestrator
â”œâ”€â”€ README.md                      # This file
â””â”€â”€ requirements.txt               # Dependencies
```

---

## ğŸ§ª QA Testing Suite

A comprehensive quality assurance framework for testing all agents in the system with **60 test questions** and **3 grader types**.

### Key Features

- **60 Test Questions**: 20 needle + 15 summary + 10 routing + 15 HITL
- **3 Grader Types**: Code-based (regex), Model-based (OpenAI), Human-in-the-loop
- **Smart Caching**: Run agents once, grade multiple times
- **Multi-format Reports**: JSON + PDF outputs

### Quick Start

```bash
# Run from main menu
python main.py  # Select option 4 (QA Testing Suite)

# Or run directly
python QA/run_qa_tests.py --test-type=all

# Use cached answers (faster, no agent re-runs)
python QA/run_qa_tests.py --cached --code-only
```

### Cost Estimation

| Mode | Tests | Duration | Cost |
|------|-------|----------|------|
| Code-only | 60 | 3 min | ~$0.03 (agents only) |
| Full Suite | 60 | 25 min | ~$0.08 (agents + graders) |
| Cached | 60 | 2 min | ~$0.05 (graders only) |

### Documentation

**See [`QA/README.md`](QA/README.md)** for comprehensive documentation including:
- Detailed test descriptions
- Grader implementations
- CLI usage and options
- Caching strategy
- Report formats
- Extending the suite

---

## ğŸ“ Additional Documentation

## ğŸ¯ Assignment Coverage

âœ… **10-page insurance claim** with chronological information  
âœ… **Needle indexing** with auto-merge (chunks â†’ parent pages)  
âœ… **Summary index** with MapReduce strategy  
âœ… **Three agents** (router, needle, summary) with specialized retrieval  
âœ… **MCP discussion** (investigated, explained limitation)  
âœ… **Agent diagram** (see `ARCHITECTURE_DIAGRAM.md`)  
âœ… **RAGAS evaluation** (6 metrics, 10 tests, Gemini judge)  
âœ… **Comprehensive documentation**  

---

## ğŸ’¡ Key Innovations

1. **Auto-Merge**: Adaptive context based on chunk patterns
2. **Always-Include Overview**: Smart summary retrieval
3. **Two-Phase Evaluation**: Efficient RAGAS workflow
4. **Production-Ready**: Error handling, config management, modular design
